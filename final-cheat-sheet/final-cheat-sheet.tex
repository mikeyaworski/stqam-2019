\documentclass[11pt]{article}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{tikz}
%\usepackage{algorithm2e}
\usetikzlibrary{arrows,automata,shapes}
\tikzstyle{block} = [rectangle, draw, fill=blue!20, 
    text width=5em, text centered, rounded corners, minimum height=2em]
\tikzstyle{bt} = [rectangle, draw, fill=blue!20, 
    text width=1em, text centered, rounded corners, minimum height=2em]

\lstset{ %
language=Java,
basicstyle=\ttfamily\scriptsize,commentstyle=\scriptsize\itshape,showstringspaces=false,breaklines=true}

\newtheorem{defn}{Definition}
\newtheorem{crit}{Criterion}

\topmargin 0pt
\advance \topmargin by -\headheight
\advance \topmargin by -\headsep
\textheight 8.9in
\oddsidemargin 0pt
\evensidemargin \oddsidemargin
\marginparwidth 0.5in
\textwidth 6.5in

\parindent 0in
\parskip 1.5ex
%\renewcommand{\baselinestretch}{1.25}

\begin{document}

\section*{Faults, Errors, and Failures}
\begin{itemize}
\item {\bf Fault} (also known as a bug): A static defect in software---incorrect lines of code.
\item {\bf Error}: An incorrect internal state---not necessarily observed yet.
\item {\bf Failure}: External, incorrect behaviour with respect to the expected behaviour---must be visible (e.g. EPIC FAIL).
\end{itemize}

\section*{RIP Fault Model}
To get from a fault to a failure:
\begin{enumerate}
\item Fault must be \emph{reachable};
\item Program state subsequent to reaching fault must be incorrect: \emph{infection}; and
\item Infected state must \emph{propagate} to output to cause a visible failure.
\end{enumerate}
Applications of the RIP model: automatic generation of test data, mutation testing.

\section*{Dealing with Faults, Errors and Failures}
Three strategies for dealing with faults are avoidance, detection and
tolerance. Or, you can just try to declare that the fault is not
a bug, if the specification is ambiguous.

\section*{Testing vs Debugging}
{\bf Testing}: evaluating software by observing its execution.\\
{\bf Debugging}: finding (and fixing) a fault given a failure.

\section*{About Testing}
We can look at testing statically or dynamically.
\paragraph{Static Testing} \hspace*{-1em} (ahead-of-time): this includes static analysis, which is typically automated and runs at compile time (or, say, nightly), as well human-driven static testing---typically code review.
\paragraph{Dynamic Testing}  \hspace*{-1em}  (at run-time): observe program behaviour by executing it; includes black-box testing (not looking at code) and white-box testing (looking at code to develop tests).

Usually the word ``testing'' means \emph{dynamic testing}.

\begin{defn}
\emph{Observability} is how easy it is to observe the system's behaviour, e.g.
its outputs, effects on the environment, hardware and software.
\end{defn}

\begin{defn}
\emph{Controlability} is how easy it is to provide the system with
needed inputs and to get the system into the right state.
\end{defn}

\begin{defn}
(Coverage Level). Given a set of test requirements TR and a test set $T$,
the coverage level is the ratio of the number of test requirements
satisfied by $T$ to the size of TR.
\end{defn}


\section*{Exploratory Testing}

\paragraph{Scenarios where Exploratory Testing Excels.} (from Bach's article)
\begin{itemize}[noitemsep]
\item providing rapid feedback on new product/feature;
\item learning product quickly;
\item diversifying testing beyond scripts;
\item finding single most important bug in shortest time;
\item independent investigation of another tester's work;
\item investigating and isolating a particular defect;
\item investigate status of a particular risk to evaluate need for scripted tests.
\end{itemize}

\paragraph{Exploratory Testing Process.}

\begin{itemize}[noitemsep]
\item Start with a charter for your testing activity, e.g. ``Explore and analyze the product elements of the software.'' 
These charters should be somewhat ambiguous.
\item Decide what area of the software to test.
\item Design a test (informally).
\item Execute the test; log bugs.
\item Repeat.
\end{itemize}

\newpage

\paragraph{Basic Blocks.} We can simplify a CFG by grouping together
statements which always execute together (in sequential programs)

\begin{defn}
A basic block is a sequence of instructions in the control-flow graph
that has one entry point and one exit point.
\end{defn}

\section*{Statement and Branch Coverage}

\begin{defn} A \emph{test path} is a path $p$ (possibly of length 0)
that starts at some initial node (i.e. in $N_0$) and ends at some final node (i.e. in $N_f$).
\end{defn}

\begin{defn}
Given a set of test requirements \emph{TR} for a graph criterion $C$, 
a test set $T$ satisfies $C$ on graph $G$ iff for every test requirement
\emph{tr} in \emph{TR}, at least one test path $p$ in $\mbox{path}(T)$ 
exists such that $p$ satisfies \emph{tr}.
\end{defn}

\paragraph{Test cases and test paths.} We connect test cases and
test paths with a mapping $\mbox{path}_G$ from test cases to test
paths; e.g. $\mbox{path}_G(t)$ is the set of test paths corresponding
to test case $t$.
\begin{itemize}[noitemsep]
\item usually we just write $\mbox{path}$ since $G$ is obvious from the context.
\item we can lift the definition of $\mbox{path}$ to test sets $T$ by defining
$\mbox{path}(T) = \{ \mbox{path}(t) | t \in T \}$.
\item each test case gives at least one test path. If the software is
  deterministic, then each test case gives exactly one test path;
  otherwise, multiple test cases may arise from one test path.
\end{itemize}

\section*{Page Objects}

A page object should abstractly represent the actions that
a user can take on a page and allow the caller to query
the state of the page (if necessary). The Selenium documentation
suggests a page object for a sign-in page.

The sign-in page object also exposes the sole functionality of the
page, which is to sign in. It then returns the page that gets loaded
after sign-in.

\newpage

\section*{Testing State Behaviour of Software via FSMs}

\begin{crit}
{\bf Complete Path Coverage}. (CPC) \emph{TR} contains all paths in $G$.
\end{crit}

Note that CPC is impossible to achieve for graphs with loops.

We propose the use of graph coverage criteria to test with FSMs.
\begin{itemize}[noitemsep]
\item nodes: software states (e.g. sets of values for key variables);
\item edges: transitions between software states, i.e. something changes
in the environment or someone enters a command.
\end{itemize}

\begin{itemize}[noitemsep]
\item node coverage: visiting every FSM state = state coverage;
\item edge coverage: visiting every FSM transition = transition coverage;
\item edge-pair coverage (extension of edge coverage to paths of length at most 2): actually useful for FSMS; transition-pair, two-trip coverage.
\end{itemize}

The next criteria are mostly not for CFGs.
\begin{defn}
A \emph{round trip} path is a path of nonzero length with no internal cycles that starts
and ends at the same node.
\end{defn}

\begin{crit}
{\bf Simple Round Trip Coverage}. (SRTC) \emph{TR} contains at least one
round-trip path for each reachable node in $G$ that begins and ends a
round-trip path.
\end{crit}

\begin{crit}
{\bf Complete Round Trip Coverage}. (CRTC) \emph{TR} contains all round-trip
paths for each reachable node in $G$.
\end{crit}

\newpage

\section*{Mutation Testing}

\paragraph{Using Grammars.} Two ways you can use input grammars for
software testing and maintenance:
\begin{itemize}[noitemsep]
\item recognizer: can include them in a program to validate inputs;
\item generator: can create program inputs for testing.
\end{itemize}

\paragraph{Some Grammar Mutation Operators.} 
\begin{itemize}
\item Nonterminal Replacement; e.g. \\
{\sf dep = {\tt "deposit"} account amount $\Longrightarrow$
 dep = {\tt "deposit"} amount amount}
(Use your judgement to replace nonterminals with similar nonterminals.)

\item Terminal Replacement; e.g. \\
{\sf amount = {\tt "\$"} digit$^+$ {\tt "."} digit \{ 2 \} 
$\Longrightarrow$ amount = {\tt "\$"} digit$^+$ {\tt "\$"} digit \{ 2 \} }

\item Terminal and Nonterminal Deletion; e.g.\\
{\sf dep = {\tt "deposit"} account amount $\Longrightarrow$
 dep = {\tt "deposit"} amount}

\item Terminal and Nonterminal Duplication; e.g.\\
{\sf dep = {\tt "deposit"} account amount $\Longrightarrow$
 dep = {\tt "deposit"} account account amount}
\end{itemize}

\paragraph{Using grammar mutation operators.}
\begin{enumerate}[noitemsep]
\item mutate grammar, generate (invalid) inputs; or,
\item use correct grammar, but mis-derive a rule once---gives ``closer''
inputs (since you only miss once.)
\end{enumerate}

\paragraph{How Fuzzing Works.} 
Two kinds of fuzzing: \emph{mutation-based} and
\emph{generation-based}. Mutation-based testing starts with
existing test cases and randomly modifies them to explore new behaviours.
Generation-based testing starts with a grammar and generates
inputs that match the grammar.

We're generating mutants $m$ for the original program $m_0$.
\begin{defn}
Test case $t$ \emph{kills} $m$ if running $t$ on $m$ gives different 
output than running $t$ on $m_0$.
\end{defn}

Mutation testing relies on two hypotheses, summarized from \cite{delgado-perez18:_evaluat_mutat_testin_nuclear_indus_case_study}.

The \emph{Competent Programmer Hypothesis} posits that programmers usually are almost right.
There may be ``subtle, low-level faults''. Mutation testing introduces faults that
are similar to such faults. (We can think of exceptions to this hypothesis---if
the code isn't tested, for instance; or, if the code was written to the wrong
requirements.)

The \emph{Coupling Effect Hypothesis} posits that complex faults are the result of
simple faults combining; hence, detecting all simple faults will detect many
complex faults.

\begin{defn}
Ground string: a (valid) string belonging to the language of the grammar (i.e. 
a programming language grammar).
\end{defn}

\begin{defn}
Mutation Operator: a rule that specifies syntactic variations of
strings generated from a grammar.
\end{defn}

\begin{defn}
Mutant: the result of one application of a mutation operator to a 
ground string.
\end{defn}

Some points:
\begin{itemize}[noitemsep]
\item How many mutation operators should you apply to get mutants? \emph{One.}
\item Should you apply every mutation operator everywhere it might apply? \emph{Too much work; choose randomly.}
\end{itemize}

\paragraph{Killing Mutants.} 
We can also define a mutation score, which is the percentage of mutants killed.

\begin{itemize}[noitemsep]
\item \emph{strong mutation}: fault must be \emph{reachable},
\emph{infect} state, and \emph{\bf propagate} to output.
\item \emph{weak mutation}: a fault which kills a mutant need only be
\emph{reachable} and \emph{infect state}.
\end{itemize}

\paragraph{Uninteresting Mutants.} Three kinds of mutants are uninteresting:
\begin{itemize}[noitemsep]
\item \emph{stillborn}: such mutants cannot compile (or immediately crash);
\item \emph{trivial}: killed by almost any test case;
\item \emph{equivalent}: indistinguishable from original program.
\end{itemize}

\paragraph{Integration Mutation.} We can go beyond mutating method bodies
by also mutating interfaces between methods, e.g.
\begin{itemize}[noitemsep]
\item change calling method by changing actual parameter values;
\item change calling method by changing callee; or
\item change callee by changing inputs and outputs.
\end{itemize}

\paragraph{Summary of Syntax-Based Testing.}~\\

\begin{tabular}{l|ll}
& Program-based & Input Space/Fuzzing \\ \hline
Grammar & Programming language & Input languages / XML \\
Summary & Mutates programs / tests integration & Input space testing \\
Use Ground String? & Yes (compare outputs) & Sometimes \\
Use Valid Strings Only? & Yes (mutants must compile) & No \\
Tests & Mutants are not tests & Mutants are tests \\
Killing & Generate tests by killing & Not applicable \\
\end{tabular}

Notes: 
\begin{itemize}[noitemsep]
\item Program-based testing has notion of strong and weak mutants; applied
exhaustively, program-based testing could subsume many other techniques.
\item Sometimes we mutate the grammar, not strings, and get tests from the
mutated grammar.
\end{itemize}

\section*{Test Design Principles}

\paragraph{Unit versus integration tests.} Unit tests are more low-level and focus on 
one particular ``class, module, or function''. They should execute quickly. Sometimes you 
need to create fake inputs (or mocks) for unit tests; we'll talk about that too. You should generally
not use an entire real input for a unit test.

\paragraph{Flaky tests are terrible.} Although your A1Q1 tests should have been 
deterministic, not all tests are deterministic. Some tests fail a small percentage
of the time. 
\begin{itemize}[noitemsep]
\item timeouts can fail when something takes surprisingly long;
\item iterators can return items in random order;
\item random number generators are, well, random.
\end{itemize}
Try really hard to make your tests not flaky. There are ways of dealing with them,
but they're not good.

\section*{Bug Finding}

A bug has got to be a violation of some specification. This might be
a language-level specification (e.g. don't dereference null pointers), or it
may be specific to an API that you are using.

\paragraph{More on Coverity.}
Coverity is a commercial product
which can find many bugs in large (millions of lines) programs; it is
therefore a leading company in building bug detection tools.
We'll talk a bit more about Coverity in the future. Clients
(900+) include organizations such as Blackberry, Yahoo, Mozilla,
MySQL, McAfee, ECI Telecom, Samsung, Siemens, Synopsys, NetApp,
Akamai, etc. These include domains including EDA, storage, security,
networking, government (NASA, JPL), embedded systems, business
applications, operating systems, and open source software.

\begin{itemize}[noitemsep]
\item Contradictions: It attempts to find lies by cross-examining; contradictions
indicate errors.
\item Deviance: It assumes programs are mostly-correct and tries to infer
  correct behaviour from that assumption. If 1 person does X, then maybe it's right,
  or maybe that was just a coincidence. But if 1000 people do X and 1 person does Y,
  the 1 person is probably wrong.
\end{itemize}

Crucially: a contradiction constitutes an error, even without knowing
the correct belief.

\paragraph{MUST-beliefs versus MAY-beliefs.} We differentiate between MUST-beliefs
(related to contradictions) and MAY-beliefs (related to deviance).

MUST-beliefs are inferred from acts that imply beliefs about the code.
For instance:
\begin{lstlisting}
  x = *p / z; // MUST: p not null
               // MUST: z != 0
  unlock(l);   // MUST: l acquired
  x++;          // MUST: x not protected by l
\end{lstlisting}

MAY-beliefs, on the other hand, could be coincidental.

\begin{center}
\begin{tabular}{l|l|l|l|l}
\begin{minipage}{5em}
  A();\\
  // ...\\
  B();
\end{minipage} &
\begin{minipage}{5em}
  A();\\
  // ...\\
  B();
\end{minipage} &
\begin{minipage}{5em}
  A();\\
  // ...\\
  B();
\end{minipage} &
\begin{minipage}{5em}
  A();\\
  // ...\\
  B();
\end{minipage} &
\begin{minipage}{20em}
// MAY: A() and B() are paired.
\end{minipage} 
\end{tabular}
\end{center}
We can check them as if they're MUST-beliefs and then rank errors
by belief confidence.

\paragraph{MUST-belief examples.} Let's look first at a couple of
MUST-beliefs about null pointers.

\begin{itemize}[noitemsep]
\item If I write {\tt *p} in a C program, I'm stating a MUST-belief that
  {\tt p} had better not be {\tt NULL}.
\item If I write the check {\tt p == NULL}, I'm implying two MUST-beliefs:
  1) POST: {\tt p} is {\tt NULL} on true path, not-{\tt NULL} on false path; 2) PRE:
  {\tt p} was unknown before the check.
\end{itemize}

\paragraph{Redundancy Checking.} 1) Code ought to do something. So,
when you have code that doesn't do anything, that's suspicious.

\paragraph{Process for verifying MAY beliefs.} We proceed as follows:
\begin{enumerate}[noitemsep]
\item    Record every successful MAY-belief check as ``check''.
\item    Record every unsucessful belief check as ``error''.
\item    Rank errors based on ``check'' : ``error'' ratio.
\end{enumerate}
Most likely errors occur when ``check'' is large, ``error'' small.

\paragraph{Summary: Belief Analysis.}
      We don't know what the right spec is.
      So, look for contradictions.

\begin{itemize}[noitemsep]
\item      MUST-beliefs: contradictions = errors!
\item      MAY-beliefs: pretend they're MUST, rank by confidence.
\end{itemize}
(A key assumption behind this belief analysis technique: most of the code is correct.)

\section*{Regression Testing}

Regression testing refers to any software testing that uncovers errors by retesting the modified program (Wikipedia). This form of testing often refers to comprehensive sets of test cases to detect regressions:
\begin{itemize}[noitemsep]
\item of bug fixes that a developer has proposed.
\item of related and unrelated other features that have been added.
\end{itemize}

Regression tests usually have the following attributes:

\begin{itemize}[noitemsep]
\item \textbf{Automated}: no real reason to have manual regression tests.
\item \textbf{Appropriately Sized}: too small and bugs will be missed. Too large and they will take a long time to run. Optimally, we want to run tests continuously.
\item \textbf{Up-to-date}: ensure that tests are valid for the version of program being tested.
\end{itemize}

\section*{Industrial Best Practices}

\begin{itemize}[noitemsep]

\item \textbf{Unit Tests:} Each class has an associated unit test. If you change a class, you must also modify the unit test.

\item \textbf{Code Reviews:} Each branch in the central version control system has owners. In order to commit code to that branch, you must have your code reviewed and approved by one of the owners. This ensures code quality.

\item \textbf{Continuous Builds:} There is often a machine that continuously checks out and tests the latest code. All unit and regression tests are run and the status is made public to the team. The status contains information about the whether the code was built successfully, whether all unit and regression tests passed, and a list of the last few commits that were made to the branch. This ensures developers try to submit good code since if you break something, everyone knows about it :)

\item \textbf{One-button Deploy:} If all tests have passed, one should be able to deploy to production
with one command.

\item \textbf{Back Button:} Systems should be designed so that it's possible to roll back changes.
\end{itemize}

\section*{State and Behaviour Tests}

Let's consider two kinds of
tests: state-based tests vs. behaviour-based tests.

\begin{itemize}[noitemsep]
\item {\bf State:} e.g. object field values.
    Verify by calling accessor methods.
  \item {\bf Behaviour:} which calls System Under Test (SUT) makes.
    Verify by inserting observation points, monitoring interactions.
\end{itemize}

In state-based tests, we inspect only outputs, and only call methods from SUT. We do not instrument the SUT. We do not check interactions.

You have two options for verifying state:
\begin{enumerate}[noitemsep]
\item procedural (bunch of asserts); or,
\item via expected objects (stay tuned).
\end{enumerate}

For behaviour verification, we can use a mock object framework (e.g. JMock) to
define expected behaviour.

\paragraph{Idea.} Observe calls to the logger,
make sure right calls happen.

\paragraph{Verifying Behaviour.} The key is to observe actions
(calls) of the SUT. Some options for doing this:
\begin{itemize}[noitemsep]
\item procedural behaviour verification
  (the challenge in that case: recording and verifying behaviour); or
\item expected behaviour specification
  (capturing the outbound calls of the SUT).
\end{itemize}

\section*{How to Improve Your Tests}

\paragraph{Reducing Test Code Duplication.}
Copy-pasting is common when writing tests. This results in
duplicate code in test cases, which has some undesirable
side effects (bloat, unnecessary asserts). We'll talk about
some techniques to mitigate duplication:
\begin{itemize}[noitemsep]
\item Expected Objects
\item Custom Assertions
\item Verification Methods
\end{itemize}

\paragraph{Benefits of Custom Assertions.}
Writing custom assertions can help with your test design.
\begin{itemize}[noitemsep]
  \item hide irrelevant detail;
  \item label actions with a good name (names are super important); and
  \item are themselves testable;
\end{itemize}

Avoid logic in tests.

\section*{Test Doubles}
Mock objects are a particular kind of test double. We need test doubles
because objects collaborate with other objects, but we only want to test
one object at a time.
Meszaros categorizes test doubles as follows:
\begin{itemize}[noitemsep]
    \item dummy objects: these are not actually test doubles; they don't do anything, but just take up space in parameter lists. Are like {\tt null}, but get past nullness checks in code.
    \item fake objects: have actual behaviour (which is correct), but somehow unsuitable for use in production; typical example is an in-memory database.
    \item stubs: produce canned answers in response to interactions from the class under test.
    \item mocks: like stubs, also produce canned answers. Difference: mock objects also check that the class under test makes the appropriate calls.
    \item spies: usually wraps the real object (instead of the mock, which stubs it), and records interactions for later verification.
\end{itemize}

\section*{Flaky Tests}

\paragraph{Dealing with flaky tests.} Companies with large test suites have found mitigations
for the flaky test suite problem. One can label known-flaky tests as flaky and automatically
re-run them to see if they eventually pass. One can also ignore or remove flaky tests.
But this is unsatisfactory: it takes a long time to re-run failing tests.

\paragraph{Causes of flakiness.} Luo et al studied 201 fixes to flaky tests in open-source
projects. They found that the three most common causes of fixable flaky tests were:
\begin{enumerate}[noitemsep]
    \item improper waits for asynchronous responses;
    \item concurrency; and
    \item test order dependency.
\end{enumerate}

\section*{Code Review}

\paragraph{Formatting.} Consistency in formatting helps avoid preventable
errors. Positioning of \{ \}s isn't something that necessarily has one
right answer. Spaces are probably better than tabs. But the most important thing is
to be self-consistent with yourself and within your project.

\begin{itemize}
\item {\bf Don't Repeat Yourself}. The usual reason for it
  being bad is that fixes in one place may remain unfixed in the other place. (Recall: it wasn't
  always bad when used for forking and templating). For instance, if February actually had 30
  days, you'd need to change a lot of code.
\item {\bf Fail Fast.} In the language of 6.031, we mean that a defect should be caught
  closest to when it's written. Static checks, as performed in compilers, catch defects
  earlier than dynamic checks, which catch defects earlier than letting wrong values percolate
  in the program state. In this particular example, there are no checks ensuring that a
  user had not permuted {\tt month} and {\tt dayOfMonth}.
\item {\bf Avoid Magic Numbers.} The above code is full of magic numbers. Particularly
  magical numbers include the {\tt 59} and {\tt 90} examples, as well as the month lengths
  and the month numbers. Instead, use names like {\tt FEBRUARY} etc. Enums are a good way
  to encode months, and days-of-months should be in an array. The {\tt 59} should really be
  {\tt 31 + 28}, or better yet, \verb!MONTH_LENGTH[JANUARY] + MONTH_LENGTH[FEBRUARY]!.
\item {\bf One Purpose Per Variable.} The specific variable that's
  being re-used in the above example is {\tt dayOfMonth}, but this
  also applies to variables that you might use in your method. Use
  different variables for different purposes. They don't cost
  anything. Best to make method parameters {\tt final} and hence
  non-modifiable.
\end{itemize}

\paragraph{Comments and code documentation}
Code should, ideally, be self-documenting, with good names for
classes, methods, and variables. Methods should come with
specifications in the form of Javadoc comments.

\section*{Reporting Bugs}

\subsection*{Properties of Important Bugs}

\begin{itemize}[noitemsep]
\item Bug is sufficiently general to affect many users (easily reproducible).
\item Bug has severe consequences (crashes, dataloss).
\item Bug is new to most recent version.
\item Bug has security implications.
\end{itemize}

\subsection*{Reproducibility}

Developers can't fix problems they can't observe. 
\begin{itemize}[noitemsep]
\item Be maximally
specific in describing steps to reproduce.
\item Write the steps down as
soon as possible, before you forget. 
\item Try to find a minimal
testcase that demonstrates the problem.
\end{itemize}

\subsection*{Anatomy of a Bug Report}

\paragraph{Summary.} Perhaps the most critical field: a one-line recap of the bug. Enables searching for and judgement of the bug.

\paragraph{Description.} Should be a complete description of the bug, 
including:

\begin{itemize}[noitemsep]
\item Overview: expanded summary, e.g. ``Drag-selecting any page crashes 
Mac builds in NSGetFactory''.
\item Steps to Reproduce (key!)
\item Actual Results: what you see when you perform the steps to reproduce.
\item Expected Results: what you think is correct
\item Build Date and Platform: on development software, helps find the bug;
include additional builds and platforms the bug might apply to.
\end{itemize}

\paragraph{Lifecycle-related fields.} Some fields summarize the
current state of the bug.

\subsection*{Properties of Good Bug Reports}
\begin{itemize}[noitemsep]
\item Reported in the database.
\item Simple: one bug per report.
\item Understandable, minimal, and generalizable.
\item Reproducible.
\item Non-judgemental. ``The developers are all morons''.
\item Not a duplicate.
\end{itemize}

\subsection*{Bug Triage}

The main attributes are type, likelihood, and priority, all evaluated on anchored scales on which staff are calibrated.

\section*{Static code analysis: PMD}

\paragraph{XPath.} Let's start from the fundamentals. You write 
\emph{selectors} to find nodes. We'll look at a simple XML document.
XPath also applies to web programming (in particular the Document
Object Model) and also to the Java code we'll be analyzing.

Here is an XML file:
\begin{lstlisting}[language=XML]
<?xml version="1.0" encoding="UTF-8"?>

<bookstore>
 <book>
  <title lang="fr">Harry Potter</title>
  <price>29.99</price>
 </book>
 <book>
  <title lang="en">Learning XML</title>
  <price>39.95</price>
 </book>
</bookstore>
\end{lstlisting}

Consider XPath expression {\tt //price}. The
result is the set of price nodes with data {\tt 29.99}, {\tt
  39.95}. So, expression {\tt //price} selects nodes with name {\tt price};
the {\tt //} means any descendants (including self) of the context node (= root node, here)---we asked for all descendants of the root named {\tt price}.
And, {\tt count(//price)} counts the number of {\tt price} elements
in the tree.

We can also specify an exact path through the tree, say
with {\tt /bookstore/book[1]/title}. This starts at the root,
visits the {\tt bookstore} element, then its first {\tt book} child,
then returns the title. If we omitted {\tt [1]}, then we'd
get all of the titles.

We can also select all elements that satisfy some condition,
e.g. {\tt /bookstore/book[price>35]/title} selects the titles
of books with price greater than 35.

Note the {\tt lang} attribute. We can select elements with a
certain value for {\tt lang}: {\tt //title[@lang="fr"]}.

In general, square brackets can contain predicates. We've 
seen pretty simple ones, but you can put arbitrary tree queries,
e.g. {\tt //price[../title[text()="Learning XML"]]}.

You can also combine predicates with {\tt and}, {\tt or}, etc.
e.g. {\tt //title[../price < 35 or @lang="en"]}. 
{\tt *} works as you might expect.

\section*{Static versus Dynamic Analysis}
{\tt valgrind} detects memory leaks dynamically.

Recall that a \emph{dynamic} analysis monitors program behaviour at runtime,
while a \emph{static} analysis reasons about the program text.

\begin{itemize}[noitemsep]
\item dynamically: You have complete information about program state on observed executions.
\item statically: You have partial information about all executions.
\end{itemize}

So how does that work out on specific examples?

\paragraph{Virtual method call resolution.} The question here is:
given a virtual method call like {\tt m.foo()}, where the actual runtime type of {\tt m} 
could vary (due to subclassing), which {\tt foo()} method actually gets called?

Dynamically, it is trivial to answer this question.

Statically, this is quite difficult. The problem is that one needs to
know the type of {\tt m}.  The easiest answer is by using Class
Hierarchy Analysis: using the declared type of {\tt m}, compute the
allowed types using the class hierarchy (i.e. subclasses of the
declared type).  Rapid Type Analysis gives a better answer---it limits
the answer to all types that are instantiated somewhere in the
program. But that requires an approximation of the reachable code,
which in turn requires the answer to the very question we're trying to
answer.  There are even more accurate algorithms which propagate type
constraints through the program.

\paragraph{Checking data structures for cycles.} Last time, we saw code
to ensure that the tree was actually acyclic. That was fairly straightforward.
Here again, static checks are difficult; they require the analysis to verify that
the code maintains the acyclicity invariant through all possible executions.

\paragraph{Unreachable code.} Not everything is easier to verify dynamically
than statically. Consider the question of whether a line of code is reachable
or not. This is relatively easy to approximate statically (that's what dead
code elimination does), but quite hard to check dynamically, since it depends
heavily on finding appropriate program inputs.

\end{document}
